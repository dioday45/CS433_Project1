{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.path.abspath(os.curdir)\n",
    "os.chdir(\"..\")\n",
    "ML_FOLDER_PATH = os.path.abspath(os.curdir)\n",
    "sys.path.append(ML_FOLDER_PATH)\n",
    "import numpy as np\n",
    "import src.helpers as hlp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the processing, we divide the train dataset into three different dataset based on the value of the feature 'PRI_jet_num'.\n",
    "We made this choice based on the fact the this feature takes values in {0, 1, 2, 3} and depending on this value, other features of the sample are undefined (noted -999.0). \n",
    "Thus, our way to deal with this is to separate the dataset in 3 (value 2 and 3 are combined together as they have the same number of defined features) and drop for each dataset the features that are not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/train.csv'\n",
    "data_0, data_1, data_2_3 = hlp.load_split_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Number of features per dataset (including index and labels) **********\n",
      "Number of defined features for dataset 0:  20\n",
      "Number of defined features for dataset 1:  24\n",
      "Number of defined features for dataset 2_3:  31\n"
     ]
    }
   ],
   "source": [
    "print('********** Number of features per dataset (including index and labels) **********')\n",
    "print('Number of defined features for dataset 0: ', data_0.shape[1])\n",
    "print('Number of defined features for dataset 1: ', data_1.shape[1])\n",
    "print('Number of defined features for dataset 2_3: ', data_2_3.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### Dealing with remaining undefined values\n",
    "Now that all features containing only undefined values are removed from each dataset, we still need to deal with the undefined values that may arrive occasionnaly for each sample.\n",
    "To deal with these remaining undefined values, we choose to replace them by the mean of the corresponding feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = hlp.nan_to_mean(data_0)\n",
    "data_1 = hlp.nan_to_mean(data_1)\n",
    "data_2_3 = hlp.nan_to_mean(data_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial expansion\n",
    "In order to increase the dimension of our dataset and have a better approximation of the relationship between the dependent and independent variable, we choose to do a polynomial expansion of the dataset.\n",
    "This means that for each sample, we will add to it $\\sum_{n=1}^d \\textbf x^d$ where $d$ is the degrees we want to add to our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = [2, 3, 4, 5, 6, 7]\n",
    "data_0 = hlp.poly_expansion(data_0, deg)\n",
    "data_1 = hlp.poly_expansion(data_1, deg)\n",
    "data_2_3 = hlp.poly_expansion(data_2_3, deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arcsinh transformation\n",
    "To deal with outliers, we choose to apply an arcsinh transformation. We choosed this particular function based on the fact that for positive values, $sinh^{-1}$ follows almost exactly the $log$ function and it has the benefit of being defined for negative values (which are present in our dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0[:, 2:] = hlp.arcsinh_transform(data_0[:, 2:])\n",
    "data_1[:, 2:] = hlp.arcsinh_transform(data_1[:, 2:])\n",
    "data_2_3[:, 2:] = hlp.arcsinh_transform(data_2_3[:, 2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0[:, 2:] = hlp.std_data(data_0[:, 2:])\n",
    "data_1[:, 2:] = hlp.std_data(data_1[:, 2:])\n",
    "data_2_3[:, 2:] = hlp.std_data(data_2_3[:, 2:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c2cef518e44103d2e87bbc7bdbad3c849c59c4ad82abf34e0b5c0b1400204ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
